{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models,Sequential\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Flatten,MaxPool2D,Activation,BatchNormalization,Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    shear_range = 0.1,\n",
    "    rotation_range = 10,\n",
    "    zoom_range = 0.1,\n",
    "    width_shift_range = 0.1,\n",
    "    height_shift_range = 0.1,\n",
    "    fill_mode = \"nearest\",\n",
    "    validation_split = 0.2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_datagen = ImageDataGenerator(rescale=1./255, validation_split =  0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 192 images belonging to 3 classes.\n",
      "Found 12 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# features_test = train_datagen.reshape(..., 28, 28, 1)      \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'mnist/train',\n",
    "    target_size = (28,28),\n",
    "    batch_size = 32,\n",
    "    classes = ['0', '1', '2'],\n",
    "    class_mode = 'categorical',\n",
    "    color_mode = 'grayscale',\n",
    "    subset = 'training') # set as training data\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    'mnist/val', # same directory as training data\n",
    "    target_size = (28, 28),\n",
    "    batch_size=32,\n",
    "    classes = ['0', '1', '2'],\n",
    "    class_mode='categorical',\n",
    "    color_mode = 'grayscale',\n",
    "    subset = 'validation') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]- Training Set\n",
      "Number of samples in train set:  192\n",
      "Number of classes in test set:  3\n",
      "Number of samples per class[train-set]:  64\n",
      "****************************************\n",
      "\n",
      "[INFO]- Validation Set\n",
      "Number of samples in validation set:  12\n",
      "Number of classes in validation set:  3\n",
      "Number of samples per class[validation-set]:  4\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO]- Training Set\")\n",
    "print(\"Number of samples in train set: \", train_generator.samples)\n",
    "print(\"Number of classes in test set: \", len(train_generator.class_indices))\n",
    "print(\"Number of samples per class[train-set]: \", int(train_generator.samples / len(train_generator.class_indices)))\n",
    "print(\"****************************************\")\n",
    "\n",
    "print(\"\\n[INFO]- Validation Set\")\n",
    "print(\"Number of samples in validation set: \", validation_generator.samples)\n",
    "print(\"Number of classes in validation set: \", len(validation_generator.class_indices))\n",
    "print(\"Number of samples per class[validation-set]: \", int(validation_generator.samples / len(validation_generator.class_indices)))\n",
    "print(\"****************************************\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 28, 28, 32)        0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 28, 28, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 14, 14, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 14, 14, 32)        0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 7, 7, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               1606144   \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 1539      \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 3)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,637,795\n",
      "Trainable params: 1,636,771\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from warnings import filters\n",
    "\n",
    "\n",
    "input_shape = (28,28,1)\n",
    "n_classes = 3\n",
    "\n",
    "model = models.Sequential([\n",
    "    keras.layers.Conv2D(32,(3,3),padding ='same',input_shape=(28,28,1)),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Conv2D(32,(3,3), activation='relu',padding ='same',input_shape=(28,28,1)),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.MaxPool2D(  pool_size=(2, 2), strides=2),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Conv2D(64,(3,3), activation='relu',padding ='same',input_shape=(28,28,1)),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.MaxPool2D(  pool_size=(2, 2), strides=2),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(512),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(n_classes),\n",
    "    keras.layers.Activation('softmax')\n",
    "    \n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "model.compile(optimizer = 'adam', loss=\"categorical_crossentropy\",metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6/6 [==============================] - 1s 216ms/step - loss: 0.1124 - accuracy: 0.9688 - val_loss: 0.6049 - val_accuracy: 0.6667\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 0.1200 - accuracy: 0.9688 - val_loss: 0.5869 - val_accuracy: 0.7500\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 1s 164ms/step - loss: 0.0674 - accuracy: 0.9792 - val_loss: 0.5819 - val_accuracy: 0.7500\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 0.1096 - accuracy: 0.9688 - val_loss: 0.5722 - val_accuracy: 0.7500\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 0.0326 - accuracy: 0.9896 - val_loss: 0.5503 - val_accuracy: 0.7500\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 0.0687 - accuracy: 0.9740 - val_loss: 0.5274 - val_accuracy: 0.7500\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 1s 142ms/step - loss: 0.0723 - accuracy: 0.9844 - val_loss: 0.5008 - val_accuracy: 0.7500\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 1s 151ms/step - loss: 0.0378 - accuracy: 0.9844 - val_loss: 0.4742 - val_accuracy: 0.9167\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 0.0310 - accuracy: 0.9896 - val_loss: 0.4346 - val_accuracy: 0.9167\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 0.0328 - accuracy: 0.9896 - val_loss: 0.3772 - val_accuracy: 0.9167\n"
     ]
    }
   ],
   "source": [
    "H = model.fit(\n",
    "    \n",
    "    train_generator, ## Numbers of training sample uing for training\n",
    "    steps_per_epoch = len(train_generator),#\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = len(validation_generator),\n",
    "    epochs = 10\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 3:\n",
    "#### train_generator:Total number of samples for used for Training.\n",
    "#### steps_per_epoch = len(train_generator):It is used to define how many batches of samples is used  in one epoch for training.which is equal to training samples / batch size , 192/32 = 6, for each epoch 6 batches will be used for training, which each batch consist of 32 images total number images will be  = 6*32\n",
    "\n",
    "###  validation_data = The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters\n",
    "   \n",
    "### validation_steps = len(validation_generator):validation_generator: Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch.\n",
    "\n",
    "### epochs = 10 : 1 epoch consist of 6 batches , which is equal to 6 * 32 = 192, for each epoch 192 times images are used for training , for 10 epochs 10*192 = 1920 times images are trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "total number of training samples/batch size  = 6.0\n",
      "192\n"
     ]
    }
   ],
   "source": [
    "print(len(train_generator)) \n",
    "#it is equal to  total number of train samples dividev by the batch size\n",
    "print(f'total number of training samples/batch size  = {192/32}')\n",
    "\n",
    "print(6*32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 35ms/step - loss: 0.6547 - accuracy: 0.8646\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(train_generator, steps=len(train_generator), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 86.46%\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.6181 - accuracy: 0.7500\n",
      "Validation (Seen) Accuracy: 75.00%\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "# Validation accuracy\n",
    "scores = model.evaluate(validation_generator, steps=len(validation_generator), verbose=1)\n",
    "print(\"Validation (Seen) Accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Saving entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 28, 28, 32)        0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 28, 28, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 14, 14, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 14, 14, 32)        0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 7, 7, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               1606144   \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 1539      \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 3)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,637,795\n",
      "Trainable params: 1,636,771\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.save('model.h5')\n",
    "# To load the saved model, use the following code:\n",
    "from tensorflow.keras.models import load_model\n",
    "loaded_model = load_model('model.h5')\n",
    "loaded_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## saving architecture and weights Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "saving_weights = model.save_weights('weights.h5')\n",
    "\n",
    "model_architecture = model.to_json()\n",
    "with open('model_architecture.json', 'w') as outfile:\n",
    "    json.dump(model_architecture, outfile)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving modle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = keras.models.load_model('model_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'float32'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saved_models.predict()\n",
    "validation_generator.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21bd38e6b50>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(validation_generator[0][0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 0, '1': 1, '2': 2}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# will show the labels names\n",
    "validation_generator.class_indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saved model prediction using unseen data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ImageDataGenerator(\n",
    "    rescale = 1/255\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "testdata = test.flow_from_directory('testimg',target_size = (28,28),\n",
    "    batch_size = 32,\n",
    "    classes = ['0', '1', '2'],\n",
    "    class_mode = 'categorical',\n",
    "    color_mode = 'grayscale',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction for 11 test images are: [0.8516847, 0.75905675, 0.8844409, 0.47941357, 0.44191352, 0.85305744, 0.7982568, 0.37979138, 0.8138467, 0.94920987, 0.91506237])\n",
      "prediction for first labels 0: [0.8516847, 0.75905675, 0.8844409, 0.47941357]\n",
      " prediction for second labels 1 : [0.44191352, 0.85305744, 0.7982568]\n",
      " prediction for third labels 3: [0.7982568, 0.37979138, 0.8138467, 0.94920987, 0.91506237]\n"
     ]
    }
   ],
   "source": [
    "# dict\n",
    "\n",
    "\n",
    "\n",
    "total_pred = []\n",
    "\n",
    "\n",
    "# labels of the test images\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "pred = saved_model.predict(testdata)\n",
    "for prediction in pred:\n",
    "    total_pred.append(prediction.max())\n",
    "print(f'prediction for 11 test images are: {total_pred})')\n",
    "\n",
    "## apped Prediction for the respective classes\n",
    "\n",
    "print(f'prediction for first labels 0: {total_pred[:4]}\\n prediction for second labels 1 : {total_pred[4:7]}\\n prediction for third labels 3: {total_pred[6:]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
